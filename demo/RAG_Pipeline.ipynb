{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/PouriaRouzrokh/RAG_Demo/blob/main/RAG_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEyKARVcnrUZ"
   },
   "source": [
    "# Retrieval Augmented Generations with Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkmz5IrNn1Qu"
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zAAOzCbaz_i9"
   },
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "  transformers==4.31.0 \\\n",
    "  sentence-transformers==2.2.2 \\\n",
    "  pinecone-client==2.2.2 \\\n",
    "  datasets==2.14.0 \\\n",
    "  accelerate==0.21.0 \\\n",
    "  einops==0.6.1 \\\n",
    "  langchain==0.0.240 \\\n",
    "  xformers==0.0.20 \\\n",
    "  bitsandbytes==0.41.0\\\n",
    "  opendatasets==0.1.22\\\n",
    "  openai==0.27.8\\\n",
    "  chromadb==0.4.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KD7GzUJ-n_7W"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from typing import List, Dict\n",
    "import warnings\n",
    "\n",
    "import langchain\n",
    "import openai\n",
    "import opendatasets as od\n",
    "import pandas as pd\n",
    "from torch import cuda, bfloat16\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nzSIVL93rOC0",
    "outputId": "afe26db5-a70c-4e60-da2a-501aa7e2f4d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your Hugging Face token: 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\n"
     ]
    }
   ],
   "source": [
    "# Configurations\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_colwidth = 1000\n",
    "\n",
    "HF_TOKEN = input('Please enter your Hugging Face token: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWmVlj46n3uj"
   },
   "source": [
    "## Data download and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "slYq5XMzpgqx"
   },
   "outputs": [],
   "source": [
    "# Remove the sample directory\n",
    "\n",
    "if os.path.exists('sample_data'):\n",
    "  shutil.rmtree('sample_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nZZGQM7Wm7Tp",
    "outputId": "fc896714-d94e-4a50-ad3c-3f310f59882a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"./questionanswer-dataset\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "# Download the \"Question-Answer Dataset\" dataset from Kaggle\n",
    "\n",
    "od.download(\"https://www.kaggle.com/datasets/rtatman/questionanswer-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "5MZdbd2-oLvT",
    "outputId": "4cd43c86-7c1e-4b22-f777-e9fd97094077"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions: 2463\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "  <div id=\"df-c1d55de3-b361-40c1-9155-e921b2ff9b44\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleTitle</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>DifficultyFromQuestioner</th>\n",
       "      <th>DifficultyFromAnswerer</th>\n",
       "      <th>ArticleFile</th>\n",
       "      <th>ï»¿ArticleTitle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alessandro_Volta</td>\n",
       "      <td>Was Alessandro Volta a professor of chemistry?</td>\n",
       "      <td>Alessandro Volta was not a professor of chemistry.</td>\n",
       "      <td>easy</td>\n",
       "      <td>easy</td>\n",
       "      <td>S10_set4_a10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alessandro_Volta</td>\n",
       "      <td>Did Alessandro Volta invent the remotely operated pistol?</td>\n",
       "      <td>Alessandro Volta did invent the remotely operated pistol.</td>\n",
       "      <td>easy</td>\n",
       "      <td>easy</td>\n",
       "      <td>S10_set4_a10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alessandro_Volta</td>\n",
       "      <td>Was Alessandro Volta taught in public schools?</td>\n",
       "      <td>Volta was taught in public schools.</td>\n",
       "      <td>easy</td>\n",
       "      <td>easy</td>\n",
       "      <td>S10_set4_a10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alessandro_Volta</td>\n",
       "      <td>Who did Alessandro Volta marry?</td>\n",
       "      <td>Alessandro Volta married Teresa Peregrini.</td>\n",
       "      <td>medium</td>\n",
       "      <td>medium</td>\n",
       "      <td>S10_set4_a10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Alessandro_Volta</td>\n",
       "      <td>What did Alessandro Volta invent in 1800?</td>\n",
       "      <td>In 1800, Alessandro Volta invented the voltaic pile.</td>\n",
       "      <td>medium</td>\n",
       "      <td>easy</td>\n",
       "      <td>S10_set4_a10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c1d55de3-b361-40c1-9155-e921b2ff9b44')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "\n",
       "\n",
       "\n",
       "    <div id=\"df-b841d43e-666f-4002-a622-bbb1f09ef351\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b841d43e-666f-4002-a622-bbb1f09ef351')\"\n",
       "              title=\"Suggest charts.\"\n",
       "              style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "    </div>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "    background-color: #E8F0FE;\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: #1967D2;\n",
       "    height: 32px;\n",
       "    padding: 0 0 0 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: #E2EBFA;\n",
       "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: #174EA6;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "    background-color: #3B4455;\n",
       "    fill: #D2E3FC;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart:hover {\n",
       "    background-color: #434B5C;\n",
       "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "    fill: #FFFFFF;\n",
       "  }\n",
       "</style>\n",
       "\n",
       "    <script>\n",
       "      async function quickchart(key) {\n",
       "        const containerElement = document.querySelector('#' + key);\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      }\n",
       "    </script>\n",
       "\n",
       "\n",
       "      <script>\n",
       "\n",
       "function displayQuickchartButton(domScope) {\n",
       "  let quickchartButtonEl =\n",
       "    domScope.querySelector('#df-b841d43e-666f-4002-a622-bbb1f09ef351 button.colab-df-quickchart');\n",
       "  quickchartButtonEl.style.display =\n",
       "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "}\n",
       "\n",
       "        displayQuickchartButton(document);\n",
       "      </script>\n",
       "      <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-c1d55de3-b361-40c1-9155-e921b2ff9b44 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-c1d55de3-b361-40c1-9155-e921b2ff9b44');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "       ArticleTitle  \\\n",
       "0  Alessandro_Volta   \n",
       "2  Alessandro_Volta   \n",
       "4  Alessandro_Volta   \n",
       "6  Alessandro_Volta   \n",
       "8  Alessandro_Volta   \n",
       "\n",
       "                                                    Question  \\\n",
       "0             Was Alessandro Volta a professor of chemistry?   \n",
       "2  Did Alessandro Volta invent the remotely operated pistol?   \n",
       "4             Was Alessandro Volta taught in public schools?   \n",
       "6                            Who did Alessandro Volta marry?   \n",
       "8                  What did Alessandro Volta invent in 1800?   \n",
       "\n",
       "                                                      Answer  \\\n",
       "0         Alessandro Volta was not a professor of chemistry.   \n",
       "2  Alessandro Volta did invent the remotely operated pistol.   \n",
       "4                        Volta was taught in public schools.   \n",
       "6                 Alessandro Volta married Teresa Peregrini.   \n",
       "8       In 1800, Alessandro Volta invented the voltaic pile.   \n",
       "\n",
       "  DifficultyFromQuestioner DifficultyFromAnswerer   ArticleFile  \\\n",
       "0                     easy                   easy  S10_set4_a10   \n",
       "2                     easy                   easy  S10_set4_a10   \n",
       "4                     easy                   easy  S10_set4_a10   \n",
       "6                   medium                 medium  S10_set4_a10   \n",
       "8                   medium                   easy  S10_set4_a10   \n",
       "\n",
       "  ï»¿ArticleTitle  \n",
       "0             NaN  \n",
       "2             NaN  \n",
       "4             NaN  \n",
       "6             NaN  \n",
       "8             NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the questions\n",
    "\n",
    "data_directory = \"/content/questionanswer-dataset\"\n",
    "df = pd.DataFrame()\n",
    "for file in ['S10_question_answer_pairs.txt','S09_question_answer_pairs.txt','S08_question_answer_pairs.txt']:\n",
    "    filename = os.path.join(data_directory, file)\n",
    "    df_tmp = pd.read_csv(filename, encoding='latin1', sep='\\t').drop_duplicates(subset=\"Question\")\n",
    "    df = pd.concat([df,df_tmp])\n",
    "\n",
    "print(f'Number of questions: {len(df)}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "mRE-Oty_qC8S",
    "outputId": "50d1966d-6359-4824-b8c0-47735ffae89c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new df length: 2190\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "  <div id=\"df-3cc26399-6b7d-4d7a-b3da-c80d03e6c3cf\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleTitle</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>DifficultyFromQuestioner</th>\n",
       "      <th>DifficultyFromAnswerer</th>\n",
       "      <th>ArticleFile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alessandro_Volta</td>\n",
       "      <td>Was Alessandro Volta a professor of chemistry?</td>\n",
       "      <td>Alessandro Volta was not a professor of chemistry.</td>\n",
       "      <td>easy</td>\n",
       "      <td>easy</td>\n",
       "      <td>S10_set4_a10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alessandro_Volta</td>\n",
       "      <td>Did Alessandro Volta invent the remotely operated pistol?</td>\n",
       "      <td>Alessandro Volta did invent the remotely operated pistol.</td>\n",
       "      <td>easy</td>\n",
       "      <td>easy</td>\n",
       "      <td>S10_set4_a10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alessandro_Volta</td>\n",
       "      <td>Was Alessandro Volta taught in public schools?</td>\n",
       "      <td>Volta was taught in public schools.</td>\n",
       "      <td>easy</td>\n",
       "      <td>easy</td>\n",
       "      <td>S10_set4_a10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alessandro_Volta</td>\n",
       "      <td>Who did Alessandro Volta marry?</td>\n",
       "      <td>Alessandro Volta married Teresa Peregrini.</td>\n",
       "      <td>medium</td>\n",
       "      <td>medium</td>\n",
       "      <td>S10_set4_a10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Alessandro_Volta</td>\n",
       "      <td>What did Alessandro Volta invent in 1800?</td>\n",
       "      <td>In 1800, Alessandro Volta invented the voltaic pile.</td>\n",
       "      <td>medium</td>\n",
       "      <td>easy</td>\n",
       "      <td>S10_set4_a10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3cc26399-6b7d-4d7a-b3da-c80d03e6c3cf')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "\n",
       "\n",
       "\n",
       "    <div id=\"df-03cc2a45-aaac-4aeb-9470-ab0b931ffaf9\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-03cc2a45-aaac-4aeb-9470-ab0b931ffaf9')\"\n",
       "              title=\"Suggest charts.\"\n",
       "              style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "    </div>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "    background-color: #E8F0FE;\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: #1967D2;\n",
       "    height: 32px;\n",
       "    padding: 0 0 0 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: #E2EBFA;\n",
       "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: #174EA6;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "    background-color: #3B4455;\n",
       "    fill: #D2E3FC;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart:hover {\n",
       "    background-color: #434B5C;\n",
       "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "    fill: #FFFFFF;\n",
       "  }\n",
       "</style>\n",
       "\n",
       "    <script>\n",
       "      async function quickchart(key) {\n",
       "        const containerElement = document.querySelector('#' + key);\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      }\n",
       "    </script>\n",
       "\n",
       "\n",
       "      <script>\n",
       "\n",
       "function displayQuickchartButton(domScope) {\n",
       "  let quickchartButtonEl =\n",
       "    domScope.querySelector('#df-03cc2a45-aaac-4aeb-9470-ab0b931ffaf9 button.colab-df-quickchart');\n",
       "  quickchartButtonEl.style.display =\n",
       "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "}\n",
       "\n",
       "        displayQuickchartButton(document);\n",
       "      </script>\n",
       "      <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-3cc26399-6b7d-4d7a-b3da-c80d03e6c3cf button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-3cc26399-6b7d-4d7a-b3da-c80d03e6c3cf');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "       ArticleTitle  \\\n",
       "0  Alessandro_Volta   \n",
       "2  Alessandro_Volta   \n",
       "4  Alessandro_Volta   \n",
       "6  Alessandro_Volta   \n",
       "8  Alessandro_Volta   \n",
       "\n",
       "                                                    Question  \\\n",
       "0             Was Alessandro Volta a professor of chemistry?   \n",
       "2  Did Alessandro Volta invent the remotely operated pistol?   \n",
       "4             Was Alessandro Volta taught in public schools?   \n",
       "6                            Who did Alessandro Volta marry?   \n",
       "8                  What did Alessandro Volta invent in 1800?   \n",
       "\n",
       "                                                      Answer  \\\n",
       "0         Alessandro Volta was not a professor of chemistry.   \n",
       "2  Alessandro Volta did invent the remotely operated pistol.   \n",
       "4                        Volta was taught in public schools.   \n",
       "6                 Alessandro Volta married Teresa Peregrini.   \n",
       "8       In 1800, Alessandro Volta invented the voltaic pile.   \n",
       "\n",
       "  DifficultyFromQuestioner DifficultyFromAnswerer   ArticleFile  \n",
       "0                     easy                   easy  S10_set4_a10  \n",
       "2                     easy                   easy  S10_set4_a10  \n",
       "4                     easy                   easy  S10_set4_a10  \n",
       "6                   medium                 medium  S10_set4_a10  \n",
       "8                   medium                   easy  S10_set4_a10  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning the original dataframe\n",
    "\n",
    "df.drop(df.columns[-1], 1, inplace=True) # We don't need the Article title column\n",
    "df.dropna(subset=['Question'], inplace=True)\n",
    "df.dropna(subset=['Answer'], inplace=True)\n",
    "\n",
    "print(f'new df length: {len(df)}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6_TcyFVXqnja",
    "outputId": "e21fc745-6aee-4b4a-f9dd-f0ce74700e57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text records: 150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('S08_set4_a5',\n",
       " 'Anders_Celsius\\n\\nAnders Celsius\\nThe observatory of Anders Celsius, from a contemporary engraving.\\nAnders Celsius (November 27, 1701   April 25, 1744) was a Swedish astronomer.\\nCelsius was born in Uppsala in Sweden. He was professor of astronomy at Uppsala University from 1730 to 1744, but traveled from 1732 to 1735 visiting notable observatories in Germany, Italy and France.\\n\\nAt Nuremberg in 1733 he published a collection of 316 observations of the aurora borealis made by himself and others over the period 1716-1732. In Paris he advocated the measurement of an arc of the meridian in Lapland, and in 1736 took part in the expedition organized for that purpose by the French Academy of Sciences, led by the French mathematician Pierre Louis Maupertuis.\\n\\nCelsius founded the Uppsala Astronomical Observatory in 1741, and in 1742 he proposed the Celsius temperature scale in a paper to the Royal Swedish Academy of Sciences. His thermometer had 100 for the freezing point of water and 0 for the boiling point. The scale was reversed by Carolus Linnaeus in 1745, to how it is today  Linnaeus\\' thermometer .\\n\\nAnders Celsius was the first to perform and publish careful experiments aiming at the definition of an international temperature scale on scientific grounds. In his Swedish paper \"Observations of two persistent degrees on a thermometer\" he reports on experiments to check that the freezing point is independent of latitude (and of atmospheric pressure). He determined the dependence of the boiling of water with atmospheric pressure (in excellent agreement with modern data). He further gave a rule for the determination of the boiling point if the barometric pressure deviates from a certain standard pressure  History of the Celsius temperature scale .\\n\\nIn 1744 he died of tuberculosis in Uppsala, and was buried in the Old Uppsala Church.\\n\\nThe Celsius crater on the Moon is named after him.\\n')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect the texts for all txt files\n",
    "\n",
    "txt_files_dir = os.path.join(data_directory, 'text_data/text_data')\n",
    "text_records: Dict = dict()\n",
    "for file_name in os.listdir(txt_files_dir):\n",
    "    if file_name.endswith('.clean'):\n",
    "        file_path = os.path.join(txt_files_dir, file_name)\n",
    "        with open(file_path, 'r', encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "            assert os.path.isfile(file_path)\n",
    "            text = f.read()\n",
    "            source = file_name.split('.')[0]\n",
    "            text_records[source] = text\n",
    "\n",
    "print(f'Number of text records: {len(text_records)}')\n",
    "list(text_records.items())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDNH7DG7uJ7_"
   },
   "source": [
    "## Regular question-answering with ChatGLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "49a85a08e85a4eeb892b49332d1379a1",
      "a8ac354ab7ea4ec48ea6aed8c15538b9",
      "c42c8ecb961a4c0ba0f880d3a85638d4",
      "1029f4fd7a554b29a21a8a1f548e4cf4",
      "2ec1c00248564463bf8cb620deebb945",
      "45d08df1ed2d4d5291679b6746b4b2ed",
      "457592a6a361448895f5c2d089729303",
      "f067cd084cf444a3aecebba9d23cd560",
      "35da18af1fec40b78e0073bd1c54b30a",
      "3dba3eb55e08414c8efcc50d02e8cf44",
      "7db7c697c94648c7b1afc425ded54b5c"
     ]
    },
    "id": "2gC2OaVVs6sb",
    "outputId": "edc984b8-2e4e-468b-b7b9-65bda903db44"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a85a08e85a4eeb892b49332d1379a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Loading the ChatGLM\n",
    "\n",
    "model_id = 'chatglm3-6b'\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "# begin initializing HF items, need auth token for these\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=HF_TOKEN\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    use_auth_token=HF_TOKEN\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CRzTIL1jv1Br"
   },
   "outputs": [],
   "source": [
    "# Setup a tokenizer\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=HF_TOKEN\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "piVWjQGmxXC9"
   },
   "outputs": [],
   "source": [
    "# LLM pipeline\n",
    "\n",
    "generator = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    task='text-generation',\n",
    "    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # mex number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")\n",
    "\n",
    "# Necessary to enable batching for inference\n",
    "generator.tokenizer.pad_token_id = generator.model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1_uDWFZFxgNt",
    "outputId": "0300e5b0-71c6-4fcd-9a33-fadcb84bb891"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "```Was Alessandro Volta taught in public schools?\n",
      "Answer in a short sentence, without explaining too much.```\n",
      "\n",
      "Response:\n",
      "```\n",
      "\n",
      "The answer is: No, Alessandro Volta was not taught in public schools.```\n"
     ]
    }
   ],
   "source": [
    "# Simple LLM inference with HuggingFace\n",
    "\n",
    "question = df.iloc[2]['Question']\n",
    "\n",
    "prompt=f\"{question}\\nAnswer in a short sentence, without explaining too much.\"\n",
    "print(f'Prompt:\\n```{prompt}```')\n",
    "\n",
    "res = generator(prompt)\n",
    "print(f'\\nResponse:\\n```{res[0][\"generated_text\"]}```')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "eWQij_zzefTY",
    "outputId": "a94ad435-8282-40e5-c60d-2aa740838a88"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n\\nThe answer is: No, Alessandro Volta was not taught in public schools.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple LLM inference with langchain\n",
    "\n",
    "lc_generator = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    return_full_text=True, # This should be true for langchain\n",
    "    task='text-generation',\n",
    "    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # mex number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")\n",
    "\n",
    "llm = langchain.llms.HuggingFacePipeline(pipeline=lc_generator)\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7TQHRNZcCxx"
   },
   "source": [
    "## RAG question-answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UOTqjSRacMdP",
    "outputId": "5721c244-5511-45d3-83ae-765b16d4b165"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 150\n",
      "Documents: page_content='Anders_Celsius\\n\\nAnders Celsius\\nThe observatory of Anders Celsius, from a contemporary engraving.\\nAnders Celsius (November 27, 1701   April 25, 1744) was a Swedish astronomer.\\nCelsius was born in Uppsala in Sweden. He was professor of astronomy at Uppsala University from 1730 to 1744, but traveled from 1732 to 1735 visiting notable observatories in Germany, Italy and France.\\n\\nAt Nuremberg in 1733 he published a collection of 316 observations of the aurora borealis made by himself and others over the period 1716-1732. In Paris he advocated the measurement of an arc of the meridian in Lapland, and in 1736 took part in the expedition organized for that purpose by the French Academy of Sciences, led by the French mathematician Pierre Louis Maupertuis.\\n\\nCelsius founded the Uppsala Astronomical Observatory in 1741, and in 1742 he proposed the Celsius temperature scale in a paper to the Royal Swedish Academy of Sciences. His thermometer had 100 for the freezing point of water and 0 for the boiling point. The scale was reversed by Carolus Linnaeus in 1745, to how it is today  Linnaeus\\' thermometer .\\n\\nAnders Celsius was the first to perform and publish careful experiments aiming at the definition of an international temperature scale on scientific grounds. In his Swedish paper \"Observations of two persistent degrees on a thermometer\" he reports on experiments to check that the freezing point is independent of latitude (and of atmospheric pressure). He determined the dependence of the boiling of water with atmospheric pressure (in excellent agreement with modern data). He further gave a rule for the determination of the boiling point if the barometric pressure deviates from a certain standard pressure  History of the Celsius temperature scale .\\n\\nIn 1744 he died of tuberculosis in Uppsala, and was buried in the Old Uppsala Church.\\n\\nThe Celsius crater on the Moon is named after him.\\n' metadata={'source': 'S08_set4_a5'}\n"
     ]
    }
   ],
   "source": [
    "# Loading text files as LangChain documents\n",
    "\n",
    "full_docs: List = list()\n",
    "for source in text_records:\n",
    "  metadata = {'source': source}\n",
    "  text = text_records[source]\n",
    "  full_docs.append(\n",
    "      langchain.docstore.document.Document(\n",
    "          page_content=text, metadata=metadata\n",
    "          )\n",
    "  )\n",
    "\n",
    "print(f'Number of documents: {len(full_docs)}')\n",
    "print(f'Documents: {full_docs[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cZzTjCdDc3wQ",
    "outputId": "2be0d22a-43a8-4b99-8573-6b33c1bc2423"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 3986\n",
      "Chunks: page_content=\"Anders_Celsius\\n\\nAnders Celsius\\nThe observatory of Anders Celsius, from a contemporary engraving.\\nAnders Celsius (November 27, 1701   April 25, 1744) was a Swedish astronomer.\\nCelsius was born in Uppsala in Sweden. He was professor of astronomy at Uppsala University from 1730 to 1744, but traveled from 1732 to 1735 visiting notable observatories in Germany, Italy and France.\\n\\nAt Nuremberg in 1733 he published a collection of 316 observations of the aurora borealis made by himself and others over the period 1716-1732. In Paris he advocated the measurement of an arc of the meridian in Lapland, and in 1736 took part in the expedition organized for that purpose by the French Academy of Sciences, led by the French mathematician Pierre Louis Maupertuis.\\n\\nCelsius founded the Uppsala Astronomical Observatory in 1741, and in 1742 he proposed the Celsius temperature scale in a paper to the Royal Swedish Academy of Sciences. His thermometer had 100 for the freezing point of water and 0 for the boiling point. The scale was reversed by Carolus Linnaeus in 1745, to how it is today  Linnaeus' thermometer .\" metadata={'source': 'S08_set4_a5'}\n"
     ]
    }
   ],
   "source": [
    "# Chunking documents\n",
    "\n",
    "def chunk_docs(\n",
    "    docs: List, chunk_size: int = 1500, chunk_overlap: int = 200\n",
    ") -> List:\n",
    "    \"\"\"Split each docuemnt into chunks, based on the chunk size and chunk_overlap.\n",
    "\n",
    "    Args:\n",
    "        docs (List[Document]): list of input docs, usually the pdf pages.\n",
    "        chunk_size (int, optional): chunk size. Defaults to 1500.\n",
    "        chunk_overlap (int, optional): chunk overlap. Defaults to 200.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: list of chunks as Documents.\n",
    "    \"\"\"\n",
    "    text_splitter = langchain.text_splitter.RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    docs = text_splitter.split_documents(docs)\n",
    "\n",
    "    return docs\n",
    "\n",
    "chunked_docs = chunk_docs(full_docs)\n",
    "\n",
    "print(f'Number of chunks: {len(chunked_docs)}')\n",
    "print(f'Chunks: {chunked_docs[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ZEmvjwFweEw9"
   },
   "outputs": [],
   "source": [
    "# Setup the embedding model\n",
    "\n",
    "embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "embed_model = langchain.embeddings.huggingface.HuggingFaceEmbeddings(\n",
    "    model_name=embed_model_id,\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'device': device, 'batch_size': 32}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7FLUiUTtl-AI",
    "outputId": "6d0b3058-a9b2-42a3-963a-9442b266274c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 5 doc embeddings, each with a dimensionality of 384.\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate the embed_model performance\n",
    "\n",
    "sample_texts = [\n",
    "    'This is sample text 1.',\n",
    "    'This is sample text 2.',\n",
    "    'This is sample text 3.',\n",
    "    'This is sample text 4.',\n",
    "    'This is sample text 5.',\n",
    "]\n",
    "\n",
    "embeddings = embed_model.embed_documents(sample_texts)\n",
    "\n",
    "print(f\"We have {len(embeddings)} doc embeddings, each with \"\n",
    "      f\"a dimensionality of {len(embeddings[0])}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "s3oO7LbThQb6"
   },
   "outputs": [],
   "source": [
    "# Setup a vector store and load it with all vector embeddings\n",
    "\n",
    "vector_db = langchain.vectorstores.Chroma.from_documents(\n",
    "    documents=chunked_docs,\n",
    "    embedding=embed_model,\n",
    "    persist_directory='./chroma_vectors',\n",
    ")\n",
    "\n",
    "vector_db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7sx8Z9B1kX_L",
    "outputId": "ace7fa24-6462-4433-d17f-1686f0000869"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Saint_Petersburg\\n\\n\\n\\nSaint Petersburg ( ,  ) is a city and a federal subject (a federal city) of Russia located on the Neva River at the head of the Gulf of Finland on the Baltic Sea. The city's other names were Petrograd ( , 1914–1924) and Leningrad ( , 1924–1991). It is often called just Petersburg ( ) and is informally known as Piter (   ).\\n\\nFounded by Tsar Peter I of Russia on 27 May 1703, it was the capital of the Russian Empire for more than two hundred years (1713–1728, 1732–1918). Saint Petersburg ceased being the capital in 1918 after the Russian Revolution of 1917. Nicholas and Alexandra: An Intimate Account of the Last of the Romanovs and the Fall of Imperial Russia (Athenum, 1967) by Robert K. Massie, ASIN B000CGP8M2 (also, Ballantine Books, 2000, ISBN 0-345-43831-0 and Black Dog & Leventhal Publishers, 2005, ISBN 1-57912-433-X)  It is Russia's second largest city after Moscow with 4.6 million inhabitants, and over 6 million people live in its vicinity. Saint Petersburg is a major European cultural centre, and an important Russian port on the Baltic Sea.\", metadata={'source': 'S10_set3_a10'}),\n",
       " Document(page_content=\"Saint_Petersburg\\n\\n\\n\\nSaint Petersburg ( ,  ) is a city and a federal subject (a federal city) of Russia located on the Neva River at the head of the Gulf of Finland on the Baltic Sea. The city's other names were Petrograd ( , 1914–1924) and Leningrad ( , 1924–1991). It is often called just Petersburg ( ) and is informally known as Piter (   ).\\n\\nFounded by Tsar Peter I of Russia on 27 May 1703, it was the capital of the Russian Empire for more than two hundred years (1713–1728, 1732–1918). Saint Petersburg ceased being the capital in 1918 after the Russian Revolution of 1917. Nicholas and Alexandra: An Intimate Account of the Last of the Romanovs and the Fall of Imperial Russia (Athenum, 1967) by Robert K. Massie, ASIN B000CGP8M2 (also, Ballantine Books, 2000, ISBN 0-345-43831-0 and Black Dog & Leventhal Publishers, 2005, ISBN 1-57912-433-X)  It is Russia's second largest city after Moscow with 4.6 million inhabitants, and over 6 million people live in its vicinity. Saint Petersburg is a major European cultural centre, and an important Russian port on the Baltic Sea.\", metadata={'source': 'S10_set3_a10'}),\n",
       " Document(page_content=\"Saint_Petersburg\\n\\n\\n\\nSaint Petersburg ( ,  ) is a city and a federal subject (a federal city) of Russia located on the Neva River at the head of the Gulf of Finland on the Baltic Sea. The city's other names were Petrograd ( , 1914–1924) and Leningrad ( , 1924–1991). It is often called just Petersburg ( ) and is informally known as Piter (   ).\\n\\nFounded by Tsar Peter I of Russia on 27 May 1703, it was the capital of the Russian Empire for more than two hundred years (1713–1728, 1732–1918). Saint Petersburg ceased being the capital in 1918 after the Russian Revolution of 1917. Nicholas and Alexandra: An Intimate Account of the Last of the Romanovs and the Fall of Imperial Russia (Athenum, 1967) by Robert K. Massie, ASIN B000CGP8M2 (also, Ballantine Books, 2000, ISBN 0-345-43831-0 and Black Dog & Leventhal Publishers, 2005, ISBN 1-57912-433-X)  It is Russia's second largest city after Moscow with 4.6 million inhabitants, and over 6 million people live in its vicinity. Saint Petersburg is a major European cultural centre, and an important Russian port on the Baltic Sea.\", metadata={'source': 'S10_set3_a10'})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demonstrating how the retriever works\n",
    "\n",
    "query = 'what city was the capital of Russia?'\n",
    "\n",
    "vector_db.similarity_search(\n",
    "    query,  # the search query\n",
    "    k=3  # returns top 3 most relevant chunks of text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "T61AVKqvklD5"
   },
   "outputs": [],
   "source": [
    "# Setting up a RAG pipeline\n",
    "\n",
    "k=5 # Number of documents to retrieve\n",
    "\n",
    "rag_pipeline = langchain.chains.RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=vector_db.as_retriever(search_kwargs = {\"k\": k})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "id": "zYoRrkU3n27C",
    "outputId": "229a7c4f-41e8-4cf2-df13-ad58c4eff401"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n\\nAnswer: The Mozart-era piano underwent significant changes during the late 18th and early 19th centuries, leading to the modern form of the instrument. Here are some key developments that took place during this period:\\n\\n1. Size and shape: Pianos became larger and more rectangular in shape, with a longer tail and a more sloping lid. This allowed for a wider range of tonal colors and dynamics.\\n2. Strings: The number of strings increased from three to four, and the wire used for the strings became thinner and more responsive. This resulted in a brighter, more agile sound.\\n3. Action: The action, or mechanism, of the piano was improved, allowing for smoother, faster playing. The hammers were made lighter and more resilient, and the dampers were made more sensitive to produce a more even tone.\\n4. Soundboard: The soundboard was enlarged and made thicker, which helped to project the sound further and with greater clarity.\\n5. Pedals: The pedals were introduced during this period, allowing the player to control the volume, tone, and overall character of the sound.\\n6. Tuning pegs: The tuning pegs were replaced by screws, making it easier to tune the piano and maintain its pitch.\\n7. Cast iron frame: The use of cast iron frames became more widespread, providing a stronger and more stable foundation for the piano.\\n\\nThese developments, along with others, contributed to the modern form of the piano and laid the groundwork for the wide range of styles and sounds that are possible on the instrument today.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic Q/A with LLM for a sample query\n",
    "\n",
    "query= \"In what years, the Mozart-era piano underwent tremendous changes that led to the modern form of the instrument?\"\n",
    "\n",
    "llm(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MCl6DyfUoihH",
    "outputId": "492eb719-13c6-43f5-eab4-a7442b80fcd8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'In what years, the Mozart-era piano underwent tremendous changes that led to the modern form of the instrument?',\n",
       " 'result': ' The Mozart-era piano underwent tremendous changes between 1790 and 1860.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG Q/A with LLM for the same query\n",
    "\n",
    "rag_pipeline(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9muVlT-NvUmt"
   },
   "source": [
    "### Conformal prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "zqJ4hG6_GdIW"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "task_list = ['college_computer_science',\n",
    "             'formal_logic',\n",
    "             'high_school_computer_science',\n",
    "             'computer_security',\n",
    "             'machine_learning',\n",
    "             'clinical_knowledge',\n",
    "             'high_school_biology',\n",
    "             'anatomy',\n",
    "             'college_chemistry',\n",
    "             'college_medicine',\n",
    "             'professional_medicine',\n",
    "             'business_ethics',\n",
    "             'professional_accounting', 'public_relations',\n",
    "             'management',\n",
    "             'marketing'\n",
    "             ]\n",
    "\n",
    "ds = load_dataset('lukaemon/mmlu', 'clinical_knowledge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d97iLZqmHags"
   },
   "outputs": [],
   "source": [
    "import prompt_questions as p\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "# List of task we consider\n",
    "task_list = ['college_computer_science', 'formal_logic', 'high_school_computer_science',\n",
    "             'computer_security', 'machine_learning',\n",
    "\n",
    "             'clinical_knowledge', 'high_school_biology', 'anatomy', 'college_chemistry',\n",
    "             'college_medicine', 'professional_medicine',\n",
    "\n",
    "             'business_ethics', 'professional_accounting', 'public_relations',\n",
    "             'management', 'marketing'\n",
    "             ]\n",
    "\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = [50278, 50279, 50277, 1, 0]\n",
    "        for stop_id in stop_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def modify_task_data(task_data, token_limit, max_size_prompt_len):\n",
    "    '''\n",
    "    task_data: load_dataset('lukaemon/mmlu', subject_name), i.e., comes from mmlu subject\n",
    "    token_limit: the maximum sized token used in forward pass (some questions are too large and thus\n",
    "    are difficult to fit into memory given we use a single A100, thus we keep a token_limit of 1500 tokens.)\n",
    "    max_size_prompt_len: Since we use 10 different prompts for one question which all differ in their one-shot\n",
    "    question, the number of total questions may become different for each prompt. Thus we chose the\n",
    "    max_size_prompt_len, which is the largest of 10 prompts, to remove questions that exceed token_limit,\n",
    "    This results in same count of questions across all 10 prompts.\n",
    "\n",
    "    Returns task_data with questions exceeding (token_limit-max_size_prompt_len) length tokens removed.\n",
    "    '''\n",
    "    new_task_data = {\n",
    "        'train': defaultdict(list),\n",
    "        'validation': defaultdict(list),\n",
    "        'test': defaultdict(list),\n",
    "    }\n",
    "    for split in new_task_data.keys():\n",
    "        for i in range(len(task_data[split])):\n",
    "            q = task_data[split]['input'][i]\n",
    "            a = task_data[split]['A'][i]\n",
    "            b = task_data[split]['B'][i]\n",
    "            c = task_data[split]['C'][i]\n",
    "            d = task_data[split]['D'][i]\n",
    "            target = task_data[split]['target'][i]\n",
    "            if len(q) + max(map(len, [a, b, c, d])) + max_size_prompt_len < token_limit:\n",
    "                new_task_data[split]['input'].append(q)\n",
    "                new_task_data[split]['A'].append(a)\n",
    "                new_task_data[split]['B'].append(b)\n",
    "                new_task_data[split]['C'].append(c)\n",
    "                new_task_data[split]['D'].append(d)\n",
    "                new_task_data[split]['target'].append(target)\n",
    "    return new_task_data\n",
    "\n",
    "\n",
    "def get_prompt(task_data, task, question_num=0, prompt_q=None):\n",
    "    '''\n",
    "    task_data:\n",
    "    Question num specifies which question will be used as prompt.\n",
    "    If prompt_q is provided, it is used as 1-shot prompt question. This\n",
    "    corresponds to GPT-4 based question prompts that we created. Else, we\n",
    "    select question corresponding to question_num from the MMLU itself to\n",
    "    generate the prompt. We select prompt from test set in this case,\n",
    "    since train set is very small sometime and may not have 10 samples.\n",
    "    We use 10 different prompts and take avergae over them to estimate\n",
    "    performance on a subject. The function returns the 1-shot question prompt.\n",
    "    '''\n",
    "\n",
    "    if prompt_q is None:\n",
    "        prompt_set = 'test'\n",
    "        if question_num > len(task_data['test']['input']) - 1:\n",
    "            print('prompt question id exceeds the length of test set')\n",
    "            print('selecting last question of the test set')\n",
    "            question_num = len(task_data['test']['input']) - 1\n",
    "        prompt_add = f'This is a question from {task.replace(\"_\", \" \")}.\\n'\n",
    "        prompt_add += f\"{task_data[prompt_set]['input'][question_num]}\\n\"\n",
    "        for letter in ['A', 'B', 'C', 'D']:\n",
    "            prompt_add += '    ' + letter + '. ' + task_data[prompt_set][letter][question_num] + '\\n'\n",
    "        prompt_add += f\"The correct answer is option: {task_data[prompt_set]['target'][question_num]}\\n\"\n",
    "    else:\n",
    "        prompt_add = f'This is a question from {task.replace(\"_\", \" \")}.'\n",
    "        prompt_add += prompt_q\n",
    "        prompt_add += '\\n'\n",
    "    prompt_add += f\"You are the world's best expert in {task.replace('_', ' ')}. \"\n",
    "    prompt_add += '''Reason step-by-step and answer the following question. '''\n",
    "    return prompt_add\n",
    "\n",
    "\n",
    "def get_question_dict(task_data, prompt_add, prompt_q_id=None):\n",
    "    '''\n",
    "    task_data: The task_data obtained after passing original mmlu dataset to modify_task_data\n",
    "    prompt_add: prompt obtained from function get_prompt (either GPT-4 based or MMLU based question\n",
    "    promots)\n",
    "    prompt_q_id: The question_id from test set in MMLU that was used to create prompt. If prompt\n",
    "    was from GPT-4 based question this is None. Else an integer specifying the question number.\n",
    "    We remove this question num from dataset since it is part of the prompt itself.\n",
    "\n",
    "    Returns:\n",
    "    questions - containing a list of dictionary where each dictionary is a (key value) pair where\n",
    "    each key is one of the option choices and each value is complete prompt+question+option string. The\n",
    "    last token in the value string of the dictionary is same as key.\n",
    "\n",
    "    answers - containing the list of answer key for each question\n",
    "\n",
    "\n",
    "    see the sample question element\n",
    "\n",
    "{'A': \"This is a question from college computer science.\\nAn integer c is a common divisor of two integers\n",
    "x and y if and only if c is a divisor of x and c is a divisor of y. Which of the following sets of integers\n",
    "could possibly be the set of all common divisors of two integers?\\n (A) {-6,-2, -1, 1, 2, 6}\\n (B) {-6, -2,\n",
    "-1, 0, 1, 2, 6}\\n (C) {-6, -3, -2, -1, 1, 2, 3, 6}\\n (D) {-6, -3, -2, -1, 0, 1, 2, 3, 6}\\nThe correct answer\n",
    "is option C.\\nYou are the world's best expert in college computer science. Reason step-by-step and answer the\n",
    "following question. The Singleton design pattern is used to guarantee that only a single instance of a class\n",
    "may be instantiated. Which of the following is (are) true of this design pattern?\\nI. The Singleton class has\n",
    "a static factory method to provide its instance.\\nII. The Singleton class can be a subclass of another class.\n",
    "\\nIII. The Singleton class has a private constructor.\\n(A) I only (B) II only (C) III only (D) I, II, and III\n",
    "\\nThe correct answer is option: A\", 'B': \"This is a question from college computer science.\\nAn integer c is\n",
    "a common divisor of two integers x and y if and only if c is a divisor of x and c is a divisor of y. Which of\n",
    "the following sets of integers could possibly be the set of all common divisors of two integers?\\n (A) {-6,-2,\n",
    "-1, 1, 2, 6}\\n (B) {-6, -2, -1, 0, 1, 2, 6}\\n (C) {-6, -3, -2, -1, 1, 2, 3, 6}\\n (D) {-6, -3, -2, -1, 0, 1, 2,\n",
    "3, 6}\\nThe correct answer is option C.\\nYou are the world's best expert in college computer science. Reason\n",
    "step-by-step and answer the following question. The Singleton design pattern is used to guarantee that only a\n",
    "single instance of a class may be instantiated. Which of the following is (are) true of this design pattern?\n",
    "\\nI. The Singleton class has a static factory method to provide its instance.\\nII. The Singleton class can be\n",
    "a subclass of another class.\\nIII. The Singleton class has a private constructor.\\n(A) I only (B) II only (C)\n",
    "III only (D) I, II, and III \\nThe correct answer is option: B\", 'C': \"This is a question from college computer\n",
    "science.\\nAn integer c is a common divisor of two integers x and y if and only if c is a divisor of x and c is\n",
    "a divisor of y. Which of the following sets of integers could possibly be the set of all common divisors of two\n",
    "integers?\\n (A) {-6,-2, -1, 1, 2, 6}\\n (B) {-6, -2, -1, 0, 1, 2, 6}\\n (C) {-6, -3, -2, -1, 1, 2, 3, 6}\\n (D)\n",
    "{-6, -3, -2, -1, 0, 1, 2, 3, 6}\\nThe correct answer is option C.\\nYou are the world's best expert in college\n",
    "computer science. Reason step-by-step and answer the following question. The Singleton design pattern is used\n",
    "to guarantee that only a single instance of a class may be instantiated. Which of the following is (are) true\n",
    "of this design pattern?\\nI. The Singleton class has a static factory method to provide its instance.\\nII. The\n",
    "Singleton class can be a subclass of another class.\\nIII. The Singleton class has a private constructor.\\n(A)\n",
    "I only (B) II only (C) III only (D) I, II, and III \\nThe correct answer is option: C\", 'D': \"This is a question\n",
    "from college computer science.\\nAn integer c is a common divisor of two integers x and y if and only if c is a\n",
    "divisor of x and c is a divisor of y. Which of the following sets of integers could possibly be the set of all\n",
    "common divisors of two integers?\\n (A) {-6,-2, -1, 1, 2, 6}\\n (B) {-6, -2, -1, 0, 1, 2, 6}\\n (C) {-6, -3, -2, -1,\n",
    "1, 2, 3, 6}\\n (D) {-6, -3, -2, -1, 0, 1, 2, 3, 6}\\nThe correct answer is option C.\\nYou are the world's best\n",
    "expert in college computer science. Reason step-by-step and answer the following question. The Singleton design\n",
    "pattern is used to guarantee that only a single instance of a class may be instantiated. Which of the following\n",
    "is (are) true of this design pattern?\\nI. The Singleton class has a static factory method to provide its\n",
    "instance.\\nII. The Singleton class can be a subclass of another class.\\nIII. The Singleton class has a private\n",
    "constructor.\\n(A) I only (B) II only (C) III only (D) I, II, and III \\nThe correct answer is option: D\"}\n",
    "\n",
    "    '''\n",
    "    questions = []\n",
    "    answers = []\n",
    "    splits = ['train', 'validation', 'test']\n",
    "    if prompt_q_id is not None:\n",
    "        print(f'Excluding test set question no {prompt_q_id} from dataset')\n",
    "\n",
    "    for split in splits:\n",
    "        if split == 'train':\n",
    "            start = 1  # In at least one subject, we found first train question to be unrelated to subject,\n",
    "            # that's why we remove question 1.\n",
    "        else:\n",
    "            start = 0\n",
    "        for i in range(start, len(task_data[split]['input'])):\n",
    "            if split == 'test' and prompt_q_id is not None:\n",
    "                if i == prompt_q_id:\n",
    "                    # Don't add prompt question to the dataset\n",
    "                    continue\n",
    "            question_dict = {}\n",
    "            # prompt_add = 'You know everything about college medicine. Answer this multiple now. Question: \\n'\n",
    "            prompt_q = prompt_add + task_data[split]['input'][i] + '\\n'\n",
    "            # prompt_q = mmlu_prompt[task] + \"\\n\\n\" + task_data['test'][i]['input'] + '\\n'\n",
    "            for letter in ['A', 'B', 'C', 'D']:\n",
    "                prompt_q += '(' + letter + ') ' + task_data[split][letter][i] + ' '\n",
    "            # prompt_q += \"\\nA: Let's think step by step.\"\n",
    "            prompt_q += \"\\nThe correct answer is option: \"\n",
    "            for letter in ['A', 'B', 'C', 'D']:\n",
    "                question_dict[letter] = prompt_q + letter\n",
    "            questions.append(question_dict)\n",
    "            answers.append(task_data[split]['target'][i])\n",
    "    return questions, answers\n",
    "\n",
    "\n",
    "def to_tokens_and_logprobs(model, tokenizer, input_texts):\n",
    "    '''\n",
    "    Takes model, tokenizer and input_texts corresponding to each of the choices\n",
    "    to do a forward pass through the model.\n",
    "    Returns log-softmax scores as a list of tuples, where first element of tuple\n",
    "    contains the option choice and second contains the corresponding log-softmax\n",
    "    score. The list has size four corresponding to the four options.\n",
    "    '''\n",
    "    all_outputs = []\n",
    "    all_input_ids = []\n",
    "    for text in input_texts:\n",
    "        input_ids = tokenizer(text, padding=True, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits.detach().cpu()\n",
    "        all_outputs.append(logits)\n",
    "        all_input_ids.append(input_ids.detach().cpu())\n",
    "        del outputs, input_ids\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    all_outputs = torch.concat(all_outputs, 0)[:, -2:-1, :]  # We take the logit corresponding to the option token\n",
    "    all_input_ids = torch.concat(all_input_ids, 0)[:, -1:]  # We also include the token id for the options\n",
    "    probs = torch.log_softmax(all_outputs.float(), dim=-1).detach().cpu()  # Log softmax scores\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    gen_probs = torch.gather(probs, 2, all_input_ids[:, :, None]).squeeze(-1)\n",
    "\n",
    "    batch = []\n",
    "    for input_sentence, input_probs in zip(all_input_ids[:, 0], gen_probs[:, 0]):\n",
    "        batch.append((tokenizer.decode(input_sentence), input_probs.item()))\n",
    "    return batch\n",
    "\n",
    "\n",
    "def softmax(logits):\n",
    "    '''\n",
    "    converts log-softmax scores to probablities.\n",
    "    '''\n",
    "    exp_logits = np.exp(logits)\n",
    "    sum_exp_logits = np.sum(exp_logits)\n",
    "    probabilities = exp_logits / sum_exp_logits\n",
    "    return probabilities\n",
    "\n",
    "\n",
    "def extract_answer(batch):\n",
    "    '''\n",
    "    converts the batch of option, log-softmax score tuples to option, probablity tuples\n",
    "    '''\n",
    "    probabilities = softmax(np.array([answer[-1] for answer in batch]))\n",
    "\n",
    "    output_with_probabilities = [(batch[i][0], probabilities[i]) for i in range(len(batch))]\n",
    "    return output_with_probabilities\n",
    "\n",
    "\n",
    "def average_question_predictions(prediction_list):\n",
    "    '''\n",
    "    Calculates the average of the probability for question-option pairs by avergaing the\n",
    "    probability across prompts.\n",
    "    '''\n",
    "    num_seeds = len(prediction_list)  # Number of random seeds (or runs)\n",
    "    average_list = []  # List to store the average predictions for each question\n",
    "\n",
    "    # Iterate through each question\n",
    "    for question_idx in range(len(prediction_list[0])):\n",
    "        # Initialize a dictionary to store the sums of probabilities for each option\n",
    "        option_sums = {'A': 0, 'B': 0, 'C': 0, 'D': 0}\n",
    "\n",
    "        # Iterate through each random seed\n",
    "        for seed_idx in range(num_seeds):\n",
    "            # Iterate through each option and its probability for the current question and seed\n",
    "            for option, value in prediction_list[seed_idx][question_idx]:\n",
    "                # Add the probability to the corresponding option sum\n",
    "                option_sums[option] += value\n",
    "\n",
    "        # Calculate the average probability for each option and store them as tuples\n",
    "        option_averages = [(key, value / num_seeds) for key, value in option_sums.items()]\n",
    "        # Add the average probabilities for the current question to the list\n",
    "        average_list.append(option_averages)\n",
    "\n",
    "    return average_list\n",
    "\n",
    "\n",
    "def accuracy(predicted_probs, correct_answers):\n",
    "    '''\n",
    "    Given predicted probability for each question-option pairs and correct answer for that question,\n",
    "    returns the accuracy.\n",
    "    '''\n",
    "    total_count = len(correct_answers)\n",
    "    assert len(correct_answers) == len(predicted_probs)\n",
    "    correct_count = 0\n",
    "\n",
    "    for i in range(total_count):\n",
    "        # Find the answer with the maximum probability for this example\n",
    "        max_prob_answer = max(predicted_probs[i], key=lambda x: x[1])[0].strip()\n",
    "        # print(max_prob_answer, correct_answers[i])\n",
    "        # Compare the predicted answer with the correct answer\n",
    "        if correct_answers[i] == max_prob_answer:\n",
    "            correct_count += 1.0\n",
    "\n",
    "    return correct_count / total_count\n",
    "\n",
    "\n",
    "def get_max_size_prompt_len(task_data, task, n=10, max_allowed_prompt_len=700):\n",
    "    '''\n",
    "    get the size of maximum length prompt out of all n prompts considered.\n",
    "    '''\n",
    "    max_len = 0\n",
    "    i = 0\n",
    "    prompt_question_ids = []\n",
    "    while len(prompt_question_ids) < n:\n",
    "        prompt_add = get_prompt(task_data, task=task, question_num=i)\n",
    "        prompt_len = len(prompt_add)\n",
    "\n",
    "        if prompt_len > max_allowed_prompt_len:\n",
    "            i += 1\n",
    "            continue\n",
    "        else:\n",
    "            prompt_question_ids.append(i)\n",
    "            i += 1\n",
    "\n",
    "        if prompt_len > max_len:\n",
    "            max_len = prompt_len\n",
    "    return max_len, prompt_question_ids\n",
    "\n",
    "\n",
    "def get_acc_index(preds, answers):\n",
    "    '''\n",
    "    Takes saved preds and answers and returns accuracy\n",
    "    '''\n",
    "    correct = 0\n",
    "    for i in range(len(preds)):\n",
    "        if preds[i].index(max(preds[i])) == answers[i]:\n",
    "            correct += 1\n",
    "    acc = correct / len(answers)\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "token_limit = 1500  # Maximum size of tokens used in forward pass.\n",
    "n = 10 # number of different MMLU based prompts used.\n",
    "task_list = task_list\n",
    "\n",
    "max_size_prompt_len_dict = {}\n",
    "prompt_question_ids_dict = {}\n",
    "for subject_name in task_list:\n",
    "    task_data = load_dataset('lukaemon/mmlu', subject_name)\n",
    "    max_len, prompt_question_ids = get_max_size_prompt_len(task_data, subject_name, n=n,\n",
    "                                                          max_allowed_prompt_len=700)\n",
    "    max_size_prompt_len_dict[subject_name] = max_len\n",
    "    prompt_question_ids_dict[subject_name] = prompt_question_ids\n",
    "\n",
    "save_dir = './llama_hf_13b'  # Model Directory\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(save_dir, low_cpu_mem_usage=True)\n",
    "model = LlamaForCausalLM.from_pretrained(save_dir, low_cpu_mem_usage=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.half().cuda()\n",
    "\n",
    "# Get prediction for subjects with MMLU based prompts\n",
    "\n",
    "acc_dicts = {}\n",
    "\n",
    "for subject_name in task_list:\n",
    "    task_data = load_dataset('lukaemon/mmlu', subject_name)\n",
    "    new_task_data = modify_task_data(task_data, token_limit, max_size_prompt_len_dict[subject_name])\n",
    "\n",
    "    acc_dicts[subject_name] = []\n",
    "    print(f'generating predictions for the subject {subject_name}')\n",
    "    for j, question_num in enumerate(prompt_question_ids_dict[subject_name]):\n",
    "        preds = []\n",
    "        targets = []\n",
    "        print(f'Running experiments with test set question_id {question_num}')\n",
    "        prompt_add = get_prompt(task_data, task=subject_name, question_num=question_num, prompt_q=None)\n",
    "        if j % 5 == 0:\n",
    "            print(prompt_add)\n",
    "        questions, answers = get_question_dict(new_task_data, prompt_q_id=question_num, prompt_add=prompt_add)\n",
    "        for i, (question, answer) in enumerate(zip(questions, answers)):\n",
    "            batch = to_tokens_and_logprobs(model, tokenizer, [v for v in question.values()])\n",
    "            torch.cuda.empty_cache()\n",
    "            preds.append(extract_answer(batch))\n",
    "            targets.append(answer)\n",
    "        print(f'Predictions Generated for {subject_name} for iteration {j}')\n",
    "        print('Calculating accuracy')\n",
    "        acc = round(accuracy(preds, targets), 3)\n",
    "        acc_dicts[subject_name].append(acc)\n",
    "        print(f'Accuracy on {subject_name} for iteration {j} is {acc:.2f} ')\n",
    "    print('*****************************************************************************************')\n",
    "    print(f'calculating average accuracy on {subject_name}')\n",
    "    print(f'Average accuracy on {subject_name} is {np.mean(np.array(acc_dicts[subject_name])):.3f}')\n",
    "    with open(\"accuracy_mmlu_prompts_10.pkl\", \"wb\") as f:\n",
    "        pickle.dump(acc_dicts, f)\n",
    "\n",
    "\n",
    "# Import GPT-4 based question prompts\n",
    "prompt_list = [p.prompt_q_list_college_cs, p.prompt_q_list_formal_logic, p.prompt_q_list_high_school_cs,\n",
    "               p.prompt_q_list_computer_security, p.prompt_q_list_machine_learning,\n",
    "\n",
    "               p.prompt_q_list_clinical_knowledge, p.prompt_q_list_high_school_bio, p.prompt_q_list_anatomy,\n",
    "               p.promtp_q_list_college_chemistry, p.prompt_q_list_college_medicine,\n",
    "               p.prompt_q_list_professional_medicine,\n",
    "\n",
    "               p.prompt_q_list_business_ethics, p.prompt_q_list_professional_accounting, p.prompt_q_list_pr,\n",
    "               p.prompt_q_list_management, p.prompt_q_list_marketing\n",
    "               ]\n",
    "\n",
    "\n",
    "prompt_list = prompt_list\n",
    "\n",
    "def get_predictions_over_n_runs(task_data, prompt_q_list, task):\n",
    "    '''\n",
    "    Takes into input mmlu dataset for a subject and list of GPT-4 based prompts for that subject\n",
    "    Returns probablity scores (as list of list) for the mmlu questions for each options (A, B, C, D)\n",
    "    and for each prompt along with the true answers along with the average accuracy over n runs.\n",
    "    '''\n",
    "    predictions_list = []\n",
    "    acc_list = []\n",
    "\n",
    "    for j, prompt_q in enumerate(prompt_q_list):\n",
    "        prompt_add = get_prompt(task_data, task=task, prompt_q=prompt_q)\n",
    "        if j % 5 == 0:\n",
    "            print(prompt_add)\n",
    "        questions, solution_answers = get_question_dict(task_data, prompt_add=prompt_add)\n",
    "        predictions = []\n",
    "        targets = []\n",
    "        for (question, answer) in zip(questions, solution_answers):\n",
    "            batch = to_tokens_and_logprobs(model, tokenizer, [v for v in question.values()])\n",
    "            torch.cuda.empty_cache()\n",
    "            predictions.append(extract_answer(batch))\n",
    "            targets.append(answer)\n",
    "        acc = round(accuracy(predictions, targets), 3)\n",
    "        print(f'Accuracy on {task} for iteration {j} is {acc:.2f} ')\n",
    "        acc_list.append(acc)\n",
    "        predictions_list.append(predictions)\n",
    "    return predictions_list, solution_answers, acc_list\n",
    "\n",
    "def get_prediction_list(subject_name, prompt_list, token_limit=1500):\n",
    "    '''\n",
    "    Runs the get_predictions_over_n_runs function for a specific subject after removing questions\n",
    "    that exceed the token limits.\n",
    "    '''\n",
    "    max_size_prompt = np.max(np.array([len(x) for x in prompt_list]))\n",
    "    task_data = load_dataset('lukaemon/mmlu', subject_name)\n",
    "    task_data_modified = modify_task_data(task_data, token_limit=token_limit,\n",
    "                                          max_size_prompt_len=max_size_prompt)\n",
    "    prediction_lists, solution_answers, avg_acc = get_predictions_over_n_runs(task_data_modified,\n",
    "                                                                     prompt_list, subject_name)\n",
    "    return prediction_lists, solution_answers, avg_acc\n",
    "\n",
    "# Get predictions for each subject using GPT-4 based prompts\n",
    "\n",
    "acc_dicts_mmlu = {}\n",
    "for task, prompt in zip(task_list, prompt_list):\n",
    "    prediction_lists, solution_answers, acc_list = get_prediction_list(task, prompt, token_limit)\n",
    "    avg_acc = np.mean(np.array(acc_list))\n",
    "    print('*****************************************************************************************')\n",
    "    print(f'calculating average accuracy on {task}')\n",
    "    print(f'Average accuracy on {task} is {avg_acc:.3f}')\n",
    "    acc_dicts_mmlu[task] = acc_list\n",
    "    with open(\"accuracy_gpt_prompts_10.pkl\", \"wb\") as f:\n",
    "        pickle.dump(acc_dicts_mmlu, f)\n",
    "    scores = np.array([[[a[1] for a in p] for p in predictions] for predictions in prediction_lists])\n",
    "\n",
    "    answer_map = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\n",
    "    targets = np.array(list(map(lambda x: answer_map[x], solution_answers)))\n",
    "    np.save(f'{task}_scores.npy', scores)\n",
    "    np.save(f'{task}_targets.npy', targets)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOnQ6t9hQsP6+oZxkLiY9YR",
   "collapsed_sections": [
    "9muVlT-NvUmt"
   ],
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1029f4fd7a554b29a21a8a1f548e4cf4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3dba3eb55e08414c8efcc50d02e8cf44",
      "placeholder": "​",
      "style": "IPY_MODEL_7db7c697c94648c7b1afc425ded54b5c",
      "value": " 3/3 [00:08&lt;00:00,  2.62s/it]"
     }
    },
    "2ec1c00248564463bf8cb620deebb945": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "35da18af1fec40b78e0073bd1c54b30a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3dba3eb55e08414c8efcc50d02e8cf44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "457592a6a361448895f5c2d089729303": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "45d08df1ed2d4d5291679b6746b4b2ed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "49a85a08e85a4eeb892b49332d1379a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a8ac354ab7ea4ec48ea6aed8c15538b9",
       "IPY_MODEL_c42c8ecb961a4c0ba0f880d3a85638d4",
       "IPY_MODEL_1029f4fd7a554b29a21a8a1f548e4cf4"
      ],
      "layout": "IPY_MODEL_2ec1c00248564463bf8cb620deebb945"
     }
    },
    "7db7c697c94648c7b1afc425ded54b5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a8ac354ab7ea4ec48ea6aed8c15538b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45d08df1ed2d4d5291679b6746b4b2ed",
      "placeholder": "​",
      "style": "IPY_MODEL_457592a6a361448895f5c2d089729303",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "c42c8ecb961a4c0ba0f880d3a85638d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f067cd084cf444a3aecebba9d23cd560",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_35da18af1fec40b78e0073bd1c54b30a",
      "value": 3
     }
    },
    "f067cd084cf444a3aecebba9d23cd560": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
